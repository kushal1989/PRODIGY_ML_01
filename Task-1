#1. Import Libraries and Dataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb
import warnings
warnings.filterwarnings("ignore")


train_data = pd.read_csv('/content/train.csv')
test_data = pd.read_csv('/content/test.csv')

train_data.head()

test_data.head()

print('Training data number = {}'.format(train_data.shape[0]))
print('Test data number = {}\n'.format(test_data.shape[0]))
train_data.columns

#2. Exploratory Data Analysis
train_data.describe()

average_sale_price = train_data['SalePrice'].mean()  # Average house price
print(f"Average sale price: {average_sale_price}")
cheapest_sale_price = train_data['SalePrice'].min()  # Price of the cheapest house
print(f"Average sale price: {cheapest_sale_price}")

average_bathrooms = int(train_data['FullBath'].mean())  # Average number of bathrooms and bedrooms
average_bedrooms = int(train_data['BedroomAbvGr'].mean())
print(f"Average number of bathrooms: {average_bathrooms}")
print(f"Average number of bedrooms: {average_bedrooms}")

max_bedrooms = train_data['BedroomAbvGr'].max()  # Maximum number of bedrooms
print(f"Maximum number of bedrooms: {max_bedrooms}")
sns.scatterplot(x = 'GrLivArea', y = 'SalePrice', data = train_data)
sns.scatterplot(x = 'TotRmsAbvGrd', y = 'SalePrice', data = train_data)
sns.scatterplot(x = 'OverallQual', y = 'SalePrice', data = train_data)
train_data.hist(bins = 20, figsize = (20,20), color = 'r');
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.histplot(data=train_data, x="SalePrice", bins=30, kde=True)
plt.title("Distribution of Sale Prices")
plt.xlabel("Sale Price")
plt.ylabel("Frequency")
plt.show()
numeric_columns = train_data.select_dtypes(include=['int64', 'float64']) # Only numeric columns for correlation analysis
missing_values = numeric_columns.isnull().sum() # Check for missing values in the numeric columns
numeric_columns = numeric_columns.fillna(0) # If there are missing values, fill them with 0
correlation_matrix = numeric_columns.corr() # Create correlation matrix

plt.figure(figsize=(20, 20))
sns.heatmap(correlation_matrix, cmap="coolwarm", annot=True)
plt.title("Correlation Heatmap")
plt.show()
plt.figure(figsize=(8, 6))
sns.boxplot(data=train_data, x="OverallQual", y="SalePrice", palette="Set3")
plt.title("Overall Quality vs. Sale Price")
plt.xlabel("Overall Quality")
plt.ylabel("Sale Price")
plt.show()

#3. Data Preprocessing
def handle_missing_values(data):
    threshold = 0.8
    data = data.dropna(thresh=len(data) * threshold, axis=1)

    numerical_cols = data.select_dtypes(include=[np.number]).columns   # Fill missing numerical values with the mean
    data[numerical_cols] = data[numerical_cols].fillna(data[numerical_cols].mean())

    categorical_cols = data.select_dtypes(exclude=[np.number]).columns # Fill missing categorical values with the most frequent value
    data[categorical_cols] = data[categorical_cols].fillna(data[categorical_cols].mode().iloc[0])

    return data

train_data = handle_missing_values(train_data)
test_data = handle_missing_values(test_data)

train_data['SalePrice'] = np.log1p(train_data['SalePrice']) # Log transform the target variable for normalization

#4. Feature Engineering
categorical_features = train_data.select_dtypes(exclude=[np.number])
print("Categorical Features:")
print("---------------------")
print(categorical_features.head())

#5. Data Splitting
X = train_data.drop('SalePrice', axis=1)
y = train_data['SalePrice']

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)

#6. Data Transformation
# Create transformers for numerical and categorical features
numerical_features = X.select_dtypes(include=[np.number]).columns
categorical_features = X.select_dtypes(exclude=[np.number]).columns

numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine transformers using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

X_train_processed = preprocessor.fit_transform(X_train)
X_valid_processed = preprocessor.transform(X_valid)
X_test_processed = preprocessor.transform(test_data)

#7. Model Selection and Training
model = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
                max_depth = 5, alpha = 10, n_estimators = 100) # Regression model (XGBoost)
model.fit(X_train_processed, y_train)

#8. Model Evaluation
y_valid_pred = model.predict(X_valid_processed)

# Evaluate the model
mse = mean_squared_error(y_valid, y_valid_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_valid, y_valid_pred)

print(f'Mean Squared Error: {mse}')
print(f'Root Mean Squared Error: {rmse}')
print(f'R-squared: {r2}')
final_model = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,
                max_depth = 5, alpha = 10, n_estimators = 100)
final_model.fit(X_train_processed, y_train)

#9. Model Evaluation on Validation Set
y_test_pred = final_model.predict(X_test_processed)
y_test_pred = np.expm1(y_test_pred)

if 'Id' in test_data.columns: # Check if 'Id' is present in the test_data columns
    submission_df = pd.DataFrame({'Id': test_data['Id'], 'SalePrice': y_test_pred})
    submission_df.to_csv('xgboost_submission.csv', index=False)
else:
    submission_df = pd.DataFrame({'SalePrice': y_test_pred}) # If 'Id' is not present, generate a default index for the submission file
    submission_df.to_csv('xgboost_submission.csv', index=False)
y_valid_pred = final_model.predict(X_valid_processed) # Predictions on the validation set

y_valid_pred = np.expm1(y_valid_pred)
y_valid_actual = np.expm1(y_valid)

mse = mean_squared_error(y_valid_actual, y_valid_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_valid_actual, y_valid_pred)
print(f'Mean Squared Error: {mse}')
print(f'Root Mean Squared Error: {rmse}')
print(f'R-squared: {r2}')

results_df = pd.DataFrame({'Actual Price': y_valid_actual, 'Predicted Price': y_valid_pred})
results_df['Price Difference'] = results_df['Actual Price'] - results_df['Predicted Price']
print(results_df.head(10))

